{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e681c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e532d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\artit\\\\IKP_2025\\\\fraud_prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1455c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d798056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\artit\\\\IKP_2025\\\\fraud_prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f4462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig: \n",
    "    root_dir : Path \n",
    "    base_model_path : Path\n",
    "    update_base_model_path : Path\n",
    "    training_data: Path\n",
    "    params_num_features : list \n",
    "    params_learning_rate : float\n",
    "    params_include_top: bool\n",
    "    params_weights: str\n",
    "    params_classes : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cf5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-23 18:20:32,342: INFO: utils: Note: NumExpr detected 22 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.]\n",
      "[2026-01-23 18:20:32,343: INFO: utils: NumExpr defaulting to 16 threads.]\n"
     ]
    }
   ],
   "source": [
    "from fraud_prediction.constants import * \n",
    "from fraud_prediction.utils.common import read_yaml, create_directories\n",
    "from fraud_prediction.entity.config_entity import TrainingConfig\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "694f9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training_config = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model \n",
    "        params = self.params \n",
    "\n",
    "        training_data = self.config.data_ingestion.unzip_dir\n",
    "        \n",
    "        create_directories([\n",
    "            Path(training_config.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir = Path(training_config.root_dir),\n",
    "            trained_model_path = Path(training_config.trained_model_path),\n",
    "            update_base_model_path=Path(prepare_base_model.update_base_model_path),\n",
    "            training_data = Path(training_data),\n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_num_features = params.NUM_FEATURES\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94c81df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba23935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import glob\n",
    "\n",
    "class Training: \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def get_base_model(self):\n",
    "        \"\"\"à¹‚à¸«à¸¥à¸” model ANN à¸—à¸µà¹ˆà¹€à¸•à¸£à¸µà¸¢à¸¡à¹„à¸§à¹‰à¸ˆà¸²à¸ stage_02\"\"\"\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.update_base_model_path\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡à¸¥à¸­à¸ˆà¸´à¸ Notebook\"\"\"\n",
    "        # 1. à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ CSV\n",
    "        data_dir = self.config.training_data\n",
    "        csv_files = glob.glob(os.path.join(data_dir, \"**/*.csv\"), recursive=True)\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"âœ… à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {df.shape[0]} à¹à¸–à¸§\")\n",
    "\n",
    "        # 2. Feature Engineering\n",
    "        df['diff_new_old_balance'] = df['newbalanceOrig'] - df['oldbalanceOrg']\n",
    "        df['diff_new_old_destiny'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    "\n",
    "        # 3. Feature Selection & One-Hot Encoding\n",
    "        cols_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud'] \n",
    "        df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "        df = pd.get_dummies(df, columns=['type'], dtype=int)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # 4. à¹à¸¢à¸ Feature à¹à¸¥à¸° Target (isFraud)\n",
    "        target_col = 'isFraud'\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "\n",
    "        # 5. à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸° Scaling\n",
    "        X_train_raw, X_valid_raw, y_train_raw, y_valid_raw = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "        X_valid_scaled = scaler.transform(X_valid_raw)\n",
    "\n",
    "        # 6. à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸¥à¸‡à¹ƒà¸™ Class Attribute (à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ float32 à¸—à¸±à¸™à¸—à¸µ)\n",
    "        self.X_train = np.asarray(X_train_scaled).astype('float32')\n",
    "        self.X_valid = np.asarray(X_valid_scaled).astype('float32')\n",
    "        self.y_train = np.asarray(y_train_raw).astype('float32')\n",
    "        self.y_valid = np.asarray(y_valid_raw).astype('float32')\n",
    "        \n",
    "        print(f\"ðŸš€ à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆ! à¸ˆà¸³à¸™à¸§à¸™ Features à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢: {self.X_train.shape[1]}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"à¹€à¸£à¸´à¹ˆà¸¡à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥ (à¹à¸¢à¸ method à¸­à¸­à¸à¸¡à¸²à¸•à¸²à¸¡ MLOps standard)\"\"\"\n",
    "        # 7. à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ Tensor à¹€à¸žà¸·à¹ˆà¸­à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ NotImplementedError\n",
    "        X_train_tensor = tf.convert_to_tensor(self.X_train, dtype=tf.float32)\n",
    "        y_train_tensor = tf.convert_to_tensor(self.y_train, dtype=tf.float32)\n",
    "        X_valid_tensor = tf.convert_to_tensor(self.X_valid, dtype=tf.float32)\n",
    "        y_valid_tensor = tf.convert_to_tensor(self.y_valid, dtype=tf.float32)\n",
    "\n",
    "        print(f\"Training with input shape: {X_train_tensor.shape}\")\n",
    "\n",
    "        self.model.fit(\n",
    "            X_train_tensor,\n",
    "            y_train_tensor,\n",
    "            epochs=self.config.params_epochs,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            validation_data=(X_valid_tensor, y_valid_tensor)\n",
    "        )\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        model.save(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e37b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-23 18:28:12,290: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-01-23 18:28:12,293: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-01-23 18:28:12,295: INFO: common: created directory at: artifacts]\n",
      "[2026-01-23 18:28:12,297: INFO: common: created directory at: artifacts\\training]\n",
      "[2026-01-23 18:28:12,341: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "âœ… à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: 636262 à¹à¸–à¸§\n",
      "ðŸš€ à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆ! à¸ˆà¸³à¸™à¸§à¸™ Features à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢: 13\n",
      "Training with input shape: (509009, 13)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[0;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[26], line 70\u001b[0m, in \u001b[0;36mTraining.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m y_valid_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_valid, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[0;32m     79\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrained_model_path,\n\u001b[0;32m     80\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\artit\\miniconda3\\envs\\fraud_final\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\artit\\miniconda3\\envs\\fraud_final\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171\u001b[0m, in \u001b[0;36mconvert_to_numpy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mRaggedTensor):\n\u001b[0;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto_tensor()\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.prepare_data()\n",
    "    training.train()\n",
    "    print(\"Training completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0c79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
