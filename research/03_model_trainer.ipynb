{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e681c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e532d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\artit\\\\IKP_2025\\\\fraud_prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1455c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d798056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\artit\\\\IKP_2025\\\\fraud_prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f4462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig: \n",
    "    root_dir : Path \n",
    "    base_model_path : Path\n",
    "    update_base_model_path : Path\n",
    "    training_data: Path\n",
    "    params_num_features : list \n",
    "    params_learning_rate : float\n",
    "    params_include_top: bool\n",
    "    params_weights: str\n",
    "    params_classes : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cf5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-26 14:48:37,405: INFO: utils: Note: NumExpr detected 22 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.]\n",
      "[2026-01-26 14:48:37,406: INFO: utils: NumExpr defaulting to 16 threads.]\n"
     ]
    }
   ],
   "source": [
    "from fraud_prediction.constants import * \n",
    "from fraud_prediction.utils.common import read_yaml, create_directories\n",
    "from fraud_prediction.entity.config_entity import TrainingConfig\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdbf85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"]=\"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694f9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training_config = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model \n",
    "        params = self.params \n",
    "\n",
    "        training_data = self.config.data_ingestion.unzip_dir\n",
    "        \n",
    "        create_directories([\n",
    "            Path(training_config.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir = Path(training_config.root_dir),\n",
    "            trained_model_path = Path(training_config.trained_model_path),\n",
    "            update_base_model_path=Path(prepare_base_model.update_base_model_path),\n",
    "            training_data = Path(training_data),\n",
    "            params_epochs = params.EPOCHS,\n",
    "            params_batch_size = params.BATCH_SIZE,\n",
    "            params_is_augmentation = params.AUGMENTATION,\n",
    "            params_num_features = params.NUM_FEATURES,\n",
    "            params_sampling_ratio=params.SAMPLING_RATIO\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c81df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eba23935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import glob\n",
    "\n",
    "class Training: \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def get_base_model(self):\n",
    "        \"\"\"‡πÇ‡∏´‡∏•‡∏î model ANN ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏à‡∏≤‡∏Å stage_02\"\"\"\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.update_base_model_path\n",
    "        )\n",
    "\n",
    "        # Re-compile immediatly with new optimizer \n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"Model Downloaded and Re-Compile already and ready for new training\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≠‡∏à‡∏¥‡∏Å Notebook\"\"\"\n",
    "        # 1. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV\n",
    "        data_dir = self.config.training_data\n",
    "        csv_files = glob.glob(os.path.join(data_dir, \"**/*.csv\"), recursive=True)\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "\n",
    "        fraud_df = df[df['isFraud'] == 1]\n",
    "        normal_df = df[df['isFraud'] == 0]\n",
    "        \n",
    "        ratio = self.config.params_sampling_ratio\n",
    "        n_normal=len(fraud_df)*ratio \n",
    "\n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏∏‡πà‡∏°‡πÑ‡∏´‡∏°\n",
    "        n_normal = min(n_normal, len(normal_df)) \n",
    "        \n",
    "        normal_downsampled = normal_df.sample(n=n_normal, random_state=42)\n",
    "        \n",
    "        df = pd.concat([fraud_df, normal_downsampled])\n",
    "        df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "        print(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô (Ratio 1:{ratio}): ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {df.shape[0]} ‡πÅ‡∏ñ‡∏ß\")\n",
    "\n",
    "        # 2. Feature Engineering\n",
    "        df['diff_new_old_balance'] = df['newbalanceOrig'] - df['oldbalanceOrg']\n",
    "        df['diff_new_old_destiny'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    "\n",
    "        # 3. Feature Selection & One-Hot Encoding\n",
    "        cols_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud','step_weeks', 'step_days'] \n",
    "        df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
    "        df = pd.get_dummies(df, columns=['type'], dtype=int)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # 4. ‡πÅ‡∏¢‡∏Å Feature ‡πÅ‡∏•‡∏∞ Target (isFraud)\n",
    "        target_col = 'isFraud'\n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "\n",
    "        # 5. Seperate Data and Scaling\n",
    "        X_train_raw, X_valid_raw, y_train_raw, y_valid_raw = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "        X_valid_scaled = scaler.transform(X_valid_raw)\n",
    "\n",
    "        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÉ‡∏ô Class Attribute (‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float32 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)\n",
    "        self.X_train = np.asarray(X_train_scaled).astype('float32')\n",
    "        self.X_valid = np.asarray(X_valid_scaled).astype('float32')\n",
    "        self.y_train = np.asarray(y_train_raw).astype('float32')\n",
    "        self.y_valid = np.asarray(y_valid_raw).astype('float32')\n",
    "        \n",
    "        print(f\" Prepared Data Done and ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Features ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {self.X_train.shape[1]}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• ANN (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏£‡∏≤‡∏á)\"\"\"\n",
    "        # 1. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Tensor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ö‡∏ô TensorFlow\n",
    "        X_train_tensor = tf.convert_to_tensor(self.X_train, dtype=tf.float32)\n",
    "        y_train_tensor = tf.convert_to_tensor(self.y_train, dtype=tf.float32)\n",
    "        X_valid_tensor = tf.convert_to_tensor(self.X_valid, dtype=tf.float32)\n",
    "        y_valid_tensor = tf.convert_to_tensor(self.y_valid, dtype=tf.float32)\n",
    "\n",
    "        print(f\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Shape: {X_train_tensor.shape}\")\n",
    "\n",
    "        # 2. Start to train \n",
    "        self.history = self.model.fit(\n",
    "            X_train_tensor,\n",
    "            y_train_tensor,\n",
    "            epochs=self.config.params_epochs,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            validation_data=(X_valid_tensor, y_valid_tensor),\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # 3. Save model\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        model.save(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cb9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e37b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-26 14:50:33,753: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-01-26 14:50:33,769: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-01-26 14:50:33,772: INFO: common: created directory at: artifacts]\n",
      "[2026-01-26 14:50:33,773: INFO: common: created directory at: artifacts\\training]\n",
      "[2026-01-26 14:50:34,001: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "Model Downloaded and Re-Compile already and ready for new training\n",
      "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô (Ratio 1:50): ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 41871 ‡πÅ‡∏ñ‡∏ß\n",
      " Prepared Data Done and ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Features ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: 13\n",
      "Start to training data 33496 transactions\n",
      "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Shape: (33496, 13)\n",
      "Epoch 1/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9412 - loss: 0.4507 - val_accuracy: 0.9804 - val_loss: 0.2359\n",
      "Epoch 2/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9804 - loss: 0.1433 - val_accuracy: 0.9804 - val_loss: 0.0939\n",
      "Epoch 3/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9804 - loss: 0.0858 - val_accuracy: 0.9804 - val_loss: 0.0810\n",
      "Epoch 4/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9804 - loss: 0.0780 - val_accuracy: 0.9804 - val_loss: 0.0751\n",
      "Epoch 5/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.9804 - loss: 0.0730 - val_accuracy: 0.9807 - val_loss: 0.0704\n",
      "[2026-01-26 14:50:43,872: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "\n",
    "    training.get_base_model()\n",
    "    training.prepare_data()\n",
    "\n",
    "    print(f\"Start to training data {training.X_train.shape[0]} transactions\")\n",
    "    training.train()\n",
    "    print(\"Training completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02467c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
